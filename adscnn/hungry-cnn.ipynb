{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Kv2It0UYTBq7"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "# TIME_FRAMES = 50\n",
    "BUFFER_SIZE = 100\n",
    "# AMOUNT_OF_FRAMES = TIME_FRAMES - BUFFER_SIZE + 1\n",
    "# AMOUNT_OF_VIDEOS = int(input(\"amount of videos: \"))\n",
    "\n",
    "WIDTH = 1280\n",
    "HEIGHT = 720\n",
    "\n",
    "frame_sq = []\n",
    "frame_sq_gray = []\n",
    "frame_sq_edges = []\n",
    "\n",
    "# dataset_no = input(\"starting number: \")\n",
    "video_count = 0\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "# rgb = np.ndarray((TIME_FRAMES - (TIME_FRAMES % BUFFER_SIZE), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_img(img1,img2):\n",
    "    img1 = normalize_gray(img1)\n",
    "    img2 = normalize_gray(img2)\n",
    "    diff = img1 - img2\n",
    "    m_norm = np.sum(abs(diff))\n",
    "#     return m_norm\n",
    "    return 0\n",
    "def normalize_frame(frame):\n",
    "    return frame/255\n",
    "\n",
    "def normalize_gray(frame):\n",
    "    rng = np.max(frame) - np.min(frame)\n",
    "    amin = np.min(frame)\n",
    "    return (frame-amin)/rng #range from [0,1]\n",
    "def createDataset(data):\n",
    "#     dataset = data[0].copy()\n",
    "#     dataset.remove(dataset[0])\n",
    "#     for datum in data:\n",
    "#         dataset = np.append(dataset, datum, axis = 1)\n",
    "#     result  = np.append(data[0],np.append(dataset[1],np.append(dataset[2],dataset[3],axis=1),axis=1),axis=1)\n",
    "    result = np.append(h,np.append(s,np.append(v,opticFlow,axis=1),axis=1),axis=1)\n",
    "    return result\n",
    "def createTSV(dataset,filename):\n",
    "    dataset = (dataset.copy()).astype(str)\n",
    "#     datasetName = \"dataset\" + str(dataset_no) + \".tsv\"\n",
    "    datasetName = filename + \".tsv\"\n",
    "    file = open(datasetName, \"w\")\n",
    "#     file.write(\"h\\ts\\tv\\tframe_edges_comp\\tframe_comp\\n\")\n",
    "    for i in range(dataset.shape[0]):\n",
    "        file.write(\"\\t\".join(dataset[i,:].tolist()))\n",
    "        file.write(\"\\n\")\n",
    "#         if(i + 1 != dataset.shape[0]):\n",
    "#             file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return\n",
    "def createTSVLabel(dataset,filename):\n",
    "    dataset = (dataset.copy()).astype(str)\n",
    "#     datasetName = \"label\" + str(dataset_no) + \".tsv\"\n",
    "    datasetName = filename\n",
    "    file = open(datasetName, \"w\")\n",
    "    file.write(\"havg\\thv\\tsavg\\tsv\\tvavg\\tvv\\tmagavg\\tmagv\\tangavg\\tangv\\n\")\n",
    "    for i in range(dataset.shape[0]):\n",
    "        file.write(\"\\t\".join(dataset[i,:].tolist()))\n",
    "        file.write(\"\\n\")\n",
    "#         if(i + 1 != dataset.shape[0]):\n",
    "#             file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Normalization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createCSV(dataset,filename):\n",
    "#     dataset = (dataset.copy()).astype(str)\n",
    "#     dataset_name = \"dataset\" + str(dataset_no) + \".csv\"\n",
    "    dataset_name = filename + \".csv\"\n",
    "    csv = open(dataset_name, \"w\")\n",
    "#     columnTitleRow = \"havg,hv,savg,sv,vavg,vv,magavg,magv,angavg,angv\\n\"\n",
    "#     csv.write(columnTitleRow)\n",
    "    for i in range(len(dataset)):\n",
    "#         csv.write(\",\".join(dataset[i,:].tolist()))\n",
    "        csv.write(\",\".join([str(v) for v in dataset[i].astype(str).tolist()]))\n",
    "#         csv.write(\"\\n\")\n",
    "    csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>STORE VIDEOS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "directory_name = input(\"directory name: \")\n",
    "os.makedirs(directory_name)\n",
    "\n",
    "BUFFER_SIZE = 100\n",
    "# AMOUNT_OF_VIDEOS = 70\n",
    "RECORD_AMOUNT = 20\n",
    "video_count = 91\n",
    "\n",
    "for i in range(RECORD_AMOUNT + 1):\n",
    "#     print(\"recording video \" + str(video_count) + \"...\")\n",
    "    sys.stdout.write(\"\\r\" + \"recording video \" + str(video_count) + \"...\")\n",
    "    sys.stdout.flush()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    WIDTH = int(cap.get(3))\n",
    "    HEIGHT = int(cap.get(4))\n",
    "    video_name = directory_name + \"/\" + str(video_count) + \".avi\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(directory_name + \"/\" + str(video_count) + '.avi',fourcc, 20.0, (640,480))\n",
    "    frame_counter = 0\n",
    "    while(True):\n",
    "        ret, frame = cap.read()\n",
    "        if (ret == True):\n",
    "            out.write(frame)\n",
    "#             cv2.imshow('frame',frame)\n",
    "            frame_counter += 1\n",
    "        if (frame_counter == BUFFER_SIZE): break\n",
    "    video_count += 1\n",
    "#     tmp = input(\"press enter to continue\")\n",
    "    cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "    tmp = input(\"press Enter\")\n",
    "video_count = 0\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ANALYZE VIDEOS + PREPROCESSING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_data(directory,x = 0):\n",
    "    directory = \"../\" + directory\n",
    "    vids = os.listdir(directory)\n",
    "    tmp = np.arange(int(x),len(vids) - 1 + int(x))\n",
    "    videos = [name + '.avi' for name in (tmp.astype(str)).tolist()]\n",
    "    videos = [directory + \"/\" + name for name in videos]\n",
    "    video_count = 0\n",
    "    AMOUNT_OF_VIDEOS = len(videos) - 1\n",
    "\n",
    "    WIDTH = 0\n",
    "    HEIGHT = 0\n",
    "\n",
    "    video_list = []\n",
    "\n",
    "    opticflow_buffer = []\n",
    "\n",
    "    print(\"then\")\n",
    "    while(video_count < len(videos)-1):\n",
    "    # while(True):\n",
    "        sys.stdout.write(\"\\r\" + \"video \" + str(videos[video_count]) + \"...\")\n",
    "        sys.stdout.flush()\n",
    "        cap = cv2.VideoCapture(videos[video_count])\n",
    "        frame_count = 0\n",
    "        data_count = 0\n",
    "        ret, frame = cap.read()\n",
    "        HEIGHT = frame.shape[0]\n",
    "        WIDTH = frame.shape[1]\n",
    "        cap.release()\n",
    "        print(\"h w\", height, width)\n",
    "        data_frame = np.ndarray((((BUFFER_SIZE - 1) // DILATION_FACTOR), HEIGHT//RESIZE_FACTOR, WIDTH//RESIZE_FACTOR, 5))\n",
    "        cap = cv2.VideoCapture(videos[video_count])\n",
    "        for i in range (BUFFER_SIZE):\n",
    "            ret, frame = cap.read()\n",
    "            frame = cv2.resize(frame, (0,0), fx=1/RESIZE_FACTOR, fy=1/RESIZE_FACTOR)\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            opticflow_buffer.append(gray)\n",
    "            if(len(opticflow_buffer) == 2):\n",
    "                hsv = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "                hsv = np.subtract(np.multiply(np.divide(hsv,255),2),1)\n",
    "                flow = cv2.calcOpticalFlowFarneback(opticflow_buffer[0],opticflow_buffer[1], None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "                mag = cv2.normalize(mag,None,-1,1,cv2.NORM_MINMAX)\n",
    "                ang = (ang*180/np.pi/2/180) - 1\n",
    "                del opticflow_buffer[0]\n",
    "                if(frame_count % DILATION_FACTOR == 0):\n",
    "                    tmp = np.ndarray((HEIGHT/RESIZE_FACTOR,WIDTH/RESIZE_FACTOR,2))\n",
    "                    tmp[:,:,0] = mag\n",
    "                    tmp[:,:,1] = ang\n",
    "                    data_frame[data_count, :, :, 0:3] = hsv\n",
    "                    data_frame[data_count, :, :, 3:5] = tmp\n",
    "                    data_count += 1\n",
    "                frame_count += 1\n",
    "        video_list.append(data_frame)\n",
    "        cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "        opticflow_buffer = []\n",
    "        video_count += 1\n",
    "#     cv2.destroyAllWindows()\n",
    "    return np.asarray(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_data_flip(directory,x = 0):\n",
    "    directory = \"../\" + directory\n",
    "    vids = os.listdir(directory)\n",
    "    tmp = np.arange(int(x),len(vids) - 1 + int(x))\n",
    "    videos = [name + '.avi' for name in (tmp.astype(str)).tolist()]\n",
    "    videos = [directory + \"/\" + name for name in videos]\n",
    "    video_count = 0\n",
    "    AMOUNT_OF_VIDEOS = len(videos) - 1\n",
    "\n",
    "    video_list = []\n",
    "    \n",
    "    while(video_count < len(videos)-1):\n",
    "    # while(True):\n",
    "        sys.stdout.write(\"\\r\" + \"video \" + str(videos[video_count]) + \"...\")\n",
    "        sys.stdout.flush()\n",
    "        cap = cv2.VideoCapture(videos[video_count])\n",
    "        ret, frame = cap.read()\n",
    "        HEIGHT = frame.shape[0]\n",
    "        WIDTH = frame.shape[1]\n",
    "        cap.release()\n",
    "        data_frame1 = process_video(videos[video_count],0)\n",
    "        data_frame2 = process_video(videos[video_count],1)\n",
    "        video_list.append(data_frame1)\n",
    "        video_list.append(data_frame2)\n",
    "        cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "        video_count += 1\n",
    "    cv2.destroyAllWindows()\n",
    "    return np.asarray(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video, is_flipped = 0):\n",
    "    frame_count = 0\n",
    "    data_count = 0\n",
    "    opticflow_buffer = []\n",
    "#     data_frame = np.ndarray((((BUFFER_SIZE - 1) // DILATION_FACTOR), HEIGHT//10, WIDTH//10, 5))\n",
    "    data_frame = np.ndarray((((BUFFER_SIZE - 1) // DILATION_FACTOR), HEIGHT//RESIZE_FACTOR,WIDTH//RESIZE_FACTOR, 5))\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    for i in range (BUFFER_SIZE):\n",
    "        ret, frame = cap.read()\n",
    "        if(is_flipped == 1):\n",
    "            frame = cv2.flip(frame, 0)\n",
    "        frame = cv2.resize(frame, (0,0), fx=1/RESIZE_FACTOR, fy=1/RESIZE_FACTOR)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        opticflow_buffer.append(gray)\n",
    "        if(len(opticflow_buffer) == 2):\n",
    "            hsv = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "            hsv = np.subtract(np.multiply(np.divide(hsv,255),2),1)\n",
    "            flow = cv2.calcOpticalFlowFarneback(opticflow_buffer[0],opticflow_buffer[1], None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "            mag = cv2.normalize(mag,None,-1,1,cv2.NORM_MINMAX)\n",
    "            ang = (ang*180/np.pi/2/180) - 1\n",
    "            del opticflow_buffer[0]\n",
    "            if(frame_count % DILATION_FACTOR == 0):\n",
    "                tmp = np.ndarray((HEIGHT//RESIZE_FACTOR,WIDTH//RESIZE_FACTOR,2))\n",
    "                tmp[:,:,0] = mag\n",
    "                tmp[:,:,1] = ang\n",
    "                data_frame[data_count, :, :, 0:3] = hsv\n",
    "                data_frame[data_count, :, :, 3:5] = tmp\n",
    "                data_count += 1\n",
    "            frame_count += 1\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPERPARAMETERS ###\n",
    "\n",
    "RESIZE_FACTOR = 5\n",
    "DILATION_FACTOR = 3\n",
    "BUFFER_SIZE = 100\n",
    "WIDTH = 640\n",
    "HEIGHT = 480\n",
    "\n",
    "#######################\n",
    "def gen_data():\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    counter = 0\n",
    "    folders = ['doit3','addition','addition2','addition3','statics','rots','frots','movs','lights','dynamiclights']\n",
    "    inits = [0,71,91,91,0,0,0,0,0,0]\n",
    "#     while(True):\n",
    "    for i in range(len(folders)):\n",
    "#         directory = input(\"Folder name: \")\n",
    "#         x = input(\"init: \")\n",
    "        directory = folders[i]\n",
    "        x = inits[i]\n",
    "        if(directory == \"\"): break\n",
    "        datum = get_data_flip(directory,x)\n",
    "        if(counter == 0):\n",
    "            dataset = datum\n",
    "            counter += 1\n",
    "        else:\n",
    "            dataset = np.append(dataset, datum, axis = 0)\n",
    "    return dataset\n",
    "def appendLabels():\n",
    "    paths = ['label4.tsv','labeladdition.tsv','labeladdition2.tsv','labeladditional3.tsv','labelstatics.tsv','labelrots.tsv',\n",
    "            'labelfrots.tsv','labelmovs.tsv','labellights.tsv','labeldynamiclights.tsv']\n",
    "    labels = []\n",
    "    pathName = input(\"Path name: \")\n",
    "    pathName = \"../\"\n",
    "#     while(True):\n",
    "    for fileName in paths:\n",
    "#         fileName = input(\"File name: \")\n",
    "        if(fileName == \"\"):\n",
    "            break\n",
    "        with open(pathName + fileName) as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            labels.append(line[:-1])\n",
    "            labels.append(line[:-1])\n",
    "    return labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video ../addition/77.avi..."
     ]
    }
   ],
   "source": [
    "dataset = gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15485356213059289289\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5892931584\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 11234165641719013764\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:02:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bone/anaconda3/envs/tensorflow/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = appendLabels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(504, 33, 96, 128, 5)\n",
      "504\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nomalize opticflow\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "dataset = np.nan_to_num(dataset)\n",
    "scaler1 = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "scaler1.fit(dataset[:,:,:,:,3].reshape(1,dataset.shape[0] * dataset.shape[1] \n",
    "                                      * dataset.shape[2] * dataset.shape[3]))\n",
    "scaler2.fit(dataset[:,:,:,:,4].reshape(1,dataset.shape[0] * dataset.shape[1] \n",
    "                                      * dataset.shape[2] * dataset.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( scaler1, open( \"mag_scaler.p\", \"wb\" ) )\n",
    "pickle.dump( scaler2, open( \"ang_scaler.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT_OF_CLASSES = 7\n",
    "TRAIN_RATIO = 0.8\n",
    "def loadLabel(dataset, labels):\n",
    "    all_labels = np.zeros((len(labels), AMOUNT_OF_CLASSES))\n",
    "    label_counter = 0\n",
    "    per_label = [0] * AMOUNT_OF_CLASSES\n",
    "    for label in labels:\n",
    "        tmp = np.zeros((1,AMOUNT_OF_CLASSES))\n",
    "        if (label == \"static\"):\n",
    "            tmp[0,0] = 1\n",
    "            per_label[0] += 1\n",
    "        elif (label == \"move\"):\n",
    "            tmp[0,1] = 1\n",
    "            per_label[1] += 1\n",
    "        elif (label == \"rot\"):\n",
    "            tmp[0,2] = 1\n",
    "            per_label[2] += 1            \n",
    "        elif (label == \"frot\"):\n",
    "            tmp[0,3] = 1\n",
    "            per_label[3] += 1\n",
    "        elif (label == \"light\"):\n",
    "            tmp[0,4] = 1\n",
    "            per_label[4] += 1\n",
    "        elif (label == \"dynamiclight\"):\n",
    "            tmp[0,5] = 1\n",
    "            per_label[5] += 1\n",
    "        else:\n",
    "            tmp[0,6] = 1\n",
    "            per_label[6] += 1\n",
    "        all_labels[label_counter,:] = tmp\n",
    "        label_counter += 1\n",
    "    dataset, all_labels = shuffle(dataset, all_labels, random_state=1)\n",
    "    return assignData(dataset, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def assignData(data,labels):\n",
    "    UNKNOWN_AMOUNT = 5\n",
    "    train_amount = int(data.shape[0] * TRAIN_RATIO)\n",
    "    AMOUNT_PER_CLASS = int((data.shape[0] - UNKNOWN_AMOUNT) * TRAIN_RATIO)\n",
    "    class_count = [0] * train_amount\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "    for i in range (data.shape[0]):\n",
    "        if(len(train_data) == train_amount):\n",
    "            test_data.append(data[i])\n",
    "            test_labels.append(labels[i])\n",
    "        else:\n",
    "            index = np.argmax(labels[i])\n",
    "            if(index == AMOUNT_OF_CLASSES - 1):\n",
    "                test_data.append(data[i])\n",
    "                test_labels.append(labels[i])\n",
    "            if( class_count[index] < AMOUNT_PER_CLASS) :\n",
    "                train_data.append(data[i])\n",
    "                train_labels.append(labels[i])\n",
    "                class_count[index] += 1\n",
    "            else:\n",
    "                test_data.append(data[i])\n",
    "                test_labels.append(abels[i])\n",
    "    return np.asarray(train_data), np.asarray(test_data), np.asarray(train_labels), np.asarray(test_labels)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def assignData_kfolds(data,labels,k=5,iteration=0):\n",
    "    UNKNOWN_AMOUNT = 5\n",
    "#     train_amount = int(data.shape[0] * TRAIN_RATIO)\n",
    "    train_amount = data.shape[0]//k\n",
    "    class_count = [0] * train_amount\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "\n",
    "    train_data = data[k * 0 : k * 0 + train_amount, :]\n",
    "    train_labels = labels[k * 0 : k * 0 + train_amount, :]\n",
    "    test_data_front = data[0 : k * 0, :]\n",
    "    test_data_back = data[k * 0 + train_amount :, :]\n",
    "    test_labels_front = labels[0 : k * 0, :]\n",
    "    test_labels_back = labels[k * 0 + train_amount :, :]\n",
    "    print(test_data_front.shape)\n",
    "    print(test_data_back.shape)\n",
    "    test_data = np.append(test_data_front, test_data_back, axis = 0)\n",
    "    test_labels = np.append(test_labels_front, test_labels_back, axis = 0)\n",
    "    return np.asarray(train_data), np.asarray(test_data), np.asarray(train_labels), np.asarray(test_labels)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT_OF_CLASSES = 7\n",
    "TRAIN_RATIO = 0.8\n",
    "def train_kfolds(dataset, labels, k=5):\n",
    "    all_labels = np.zeros((len(labels), AMOUNT_OF_CLASSES))\n",
    "    label_counter = 0\n",
    "    per_label = [0] * AMOUNT_OF_CLASSES\n",
    "    for label in labels:\n",
    "        tmp = np.zeros((1,AMOUNT_OF_CLASSES))\n",
    "        if (label == \"static\"):\n",
    "            tmp[0,0] = 1\n",
    "            per_label[0] += 1\n",
    "        elif (label == \"move\"):\n",
    "            tmp[0,1] = 1\n",
    "            per_label[1] += 1\n",
    "        elif (label == \"rot\"):\n",
    "            tmp[0,2] = 1\n",
    "            per_label[2] += 1            \n",
    "        elif (label == \"frot\"):\n",
    "            tmp[0,3] = 1\n",
    "            per_label[3] += 1\n",
    "        elif (label == \"light\"):\n",
    "            tmp[0,4] = 1\n",
    "            per_label[4] += 1\n",
    "        elif (label == \"dynamiclight\"):\n",
    "            tmp[0,5] = 1\n",
    "            per_label[5] += 1\n",
    "        else:\n",
    "            tmp[0,6] = 1\n",
    "            per_label[6] += 1\n",
    "        all_labels[label_counter,:] = tmp\n",
    "        label_counter += 1\n",
    "    dataset, all_labels = shuffle(dataset, all_labels, random_state=1)\n",
    "    acc_sum = 0\n",
    "    for iteration in range (k):\n",
    "        x_train, x_test, y_train, y_test = assignData_kfolds(dataset, all_labels, 5, iteration)\n",
    "        model = gen_model()\n",
    "        opti = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=opti,metrics=['accuracy'])\n",
    "        model.fit(x_train, y_train, batch_size=4,validation_split=0.2,epochs=10, verbose=1)\n",
    "        score = model.evaluate(x_test, y_test, batch_size = 4, verbose=0)\n",
    "        print(\"iteration \" + str(iteration) + \"score :\")\n",
    "        print(score)\n",
    "        acc_sum += score[1]\n",
    "    print(\"average score: \", acc_sum / k)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kfolds(dataset, labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = loadLabel(dataset,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(403, 33, 96, 128, 5)\n",
      "(403, 7)\n",
      "(138, 33, 96, 128, 5)\n",
      "(138, 7)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0,0].shape)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution3D, MaxPooling3D\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "\n",
    "def gen_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Convolution3D(20,(5,5,5),strides=1,data_format='channels_last',input_shape = (33,96,128,5),activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2), padding='VALID'))\n",
    "    model.add(Convolution3D(50,(5,5,5),strides=1,data_format='channels_last',activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "# #     model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2), padding='VALID'))\n",
    "    model.add(Convolution3D(100,(5,5,5),strides=1,data_format='channels_last',activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling3D(pool_size=(1,2,2), padding='VALID'))\n",
    "# #     model.add(Convolution3D(70,(2,5,5),strides=1,activation='relu'))\n",
    "# #     model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_16 (Conv3D)           (None, 29, 92, 124, 20)   12520     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 14, 46, 62, 20)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_17 (Conv3D)           (None, 10, 42, 58, 50)    125050    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 42, 58, 50)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_17 (MaxPooling (None, 5, 21, 29, 50)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 1, 17, 25, 100)    625100    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 17, 25, 100)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_18 (MaxPooling (None, 1, 8, 12, 100)     0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               1228928   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 1,992,501\n",
      "Trainable params: 1,992,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = gen_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "opti = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opti,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 322 samples, validate on 81 samples\n",
      "Epoch 1/10\n",
      "322/322 [==============================] - 10s 31ms/step - loss: 1.6924 - acc: 0.3106 - val_loss: 1.4014 - val_acc: 0.4444\n",
      "Epoch 2/10\n",
      "322/322 [==============================] - 9s 29ms/step - loss: 1.2392 - acc: 0.5186 - val_loss: 1.3661 - val_acc: 0.6049\n",
      "Epoch 3/10\n",
      "322/322 [==============================] - 10s 30ms/step - loss: 1.0060 - acc: 0.6398 - val_loss: 1.2878 - val_acc: 0.5802\n",
      "Epoch 4/10\n",
      "322/322 [==============================] - 10s 30ms/step - loss: 0.8560 - acc: 0.6863 - val_loss: 1.1688 - val_acc: 0.6543\n",
      "Epoch 5/10\n",
      "322/322 [==============================] - 10s 30ms/step - loss: 0.6108 - acc: 0.7919 - val_loss: 0.9543 - val_acc: 0.6667\n",
      "Epoch 6/10\n",
      "322/322 [==============================] - 10s 30ms/step - loss: 0.4315 - acc: 0.8292 - val_loss: 0.8911 - val_acc: 0.7037\n",
      "Epoch 7/10\n",
      "322/322 [==============================] - 10s 30ms/step - loss: 0.3516 - acc: 0.8602 - val_loss: 0.7008 - val_acc: 0.7407\n",
      "Epoch 8/10\n",
      "322/322 [==============================] - 10s 30ms/step - loss: 0.2548 - acc: 0.9161 - val_loss: 0.7337 - val_acc: 0.7037\n",
      "Epoch 9/10\n",
      "322/322 [==============================] - 10s 30ms/step - loss: 0.1299 - acc: 0.9565 - val_loss: 0.6584 - val_acc: 0.7778\n",
      "Epoch 10/10\n",
      "322/322 [==============================] - 10s 31ms/step - loss: 0.1281 - acc: 0.9627 - val_loss: 0.8707 - val_acc: 0.7284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f354e6a65c0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          batch_size=4,validation_split=0.2,epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          batch_size=4, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 29, 92, 124, 20)\n"
     ]
    }
   ],
   "source": [
    "print(layer.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def f1_score():\n",
    "# for datum in range (x_test.shape[0]):\n",
    "    preds_test = [argmax(model.predict(x)) for x in da]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7599216087626807, 0.7463768115942029]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size = 4, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "print(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = ['static', 'move', 'rot', 'frot', 'light', 'dynamiclight','unknown']\n",
    "\n",
    "for test_num in range (50,70):\n",
    "#     lol = normalize(train_data[test_num].reshape(1,-1))\n",
    "    lol = x_test[test_num].reshape(1,33,48,64,5)\n",
    "    lol_label = y_test[test_num]\n",
    "    # print(lol.shape)\n",
    "    # print(lol, lol_label)\n",
    "    prediction = model.predict(lol)\n",
    "    print(definition[np.argmax(prediction[0])])\n",
    "    print(definition[np.argmax(lol_label)])\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = []\n",
    "ground_test = []\n",
    "preds_train = []\n",
    "ground_train = []\n",
    "\n",
    "for i in range (x_test.shape[0]):\n",
    "    datum = x_test[i].reshape(1,33,48,64,5)\n",
    "    ground_test.append(np.argmax(y_test[i,:]))\n",
    "    preds_test.append(np.argmax(model.predict(datum)[0]))\n",
    "for i in range (x_train.shape[0]):\n",
    "    datum = x_train[i].reshape(1,33,48,64,5)\n",
    "    ground_train.append(np.argmax(y_train[i,:]))\n",
    "    preds_train.append(np.argmax(model.predict(datum)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix_test = confusion_matrix(ground_test, preds_test)\n",
    "cnf_matrix_train = confusion_matrix(ground_train, preds_train)\n",
    "np.set_printoptions(precision=2)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# # Plot non-normalized confusion matrix\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=definition,\n",
    "#                       title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_test, classes=definition, normalize=True,\n",
    "                      title='Normalized test confusion matrix')\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix_train, classes=definition, normalize=True,\n",
    "                      title='Normalized train confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "ASD Vision measure.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
